---
title: "Classify"
format: html
editor: visual
---

## ‚ùî How to use this notebook:

This is a Quarto notebook, written in R, designed to facilitate development of acoustic classifier models from .csv formatted training data. Quarto notebooks consist of code cells, which are shaded grey and can be run by clicking the green triangle at their upper right corners. Running a cell causes any code within to be executed and any outputs from the code will be displayed below the cell.

**Run the cell below to see its output:**

```{r}
x <- 'cows are black and white.'
print(x)
```

A benefit of this format is that both the code and its output are visible simultaneously. Code can be adjusted to change parameters and labels as needed.

**Try changing the variable INSERT below:**

```{r}
x <- 'cows are black and white.'
INSERT <- 'Some'
print(paste(INSERT, x))
```

## ‚öôÔ∏è (1) Setting up

The first steps for this analysis are to source the functions needed for the analysis and to read in your .csv formatted data.

**Run the cell below to source required functions:**

```{r}
source('Classify_functions.R')
```

Your .csv formatted data should be structured in a hierarchy of folders, where levels of the hierarchy represent different types of information such as location, species, year, etc. The example below shows a hierarchical data structure of folders organized by species, location, and recording. The .csv data should be kept in the lowest level of the structure (e.g. recording).

#### **‚îî‚îÄ‚îÄ** SpeciesA

#### Location1

Recording01-01-2025

Recording04-01-2025

Recording06-01-2025

Recording07-01-2025

#### Location2

Recording14-01-2025

Recording15-01-2025

#### **‚îî‚îÄ‚îÄ** SpeciesB

#### Location2

Recording25-01-2025

#### Location3

Recording27-01-2025

#### **‚îî‚îÄ‚îÄ** SpeciesC

#### Location1

Recording09-01-2025

Recording11-01-2025

Recording31-01-2025

Before reading in your data using the *loadDataFromHier()* function in the cell below, make sure that you specify the following required variables:

-   **root_directory** \| folder where your data is located. [Example:]{.underline} "C:/Users/Bob/Desktop/Data"

-   **from_folders** \| set as TRUE or FALSE to indicate whether or not your .csv data is stored in a hierarchical folder structure like the example above.

-   **levels** \| vector of labels for each folder level in your hierarchical structure. If **from_folders** is FALSE, this will default to using the "KnownSpecies" and "EncounterID" variables in your .csv data.

-   **startVar** \| name of the first variable (in terms of column number) in your data. [Example]{.underline}: "FREQMAX"

-   **endVar** \| name of the last variable in your data. [Example:]{.underline} "STEPDUR".

-   **omitVars** \| vector of variables to omit from the analysis. [Example:]{.underline} "DURATION".

-   **filterVars_min** \| list of minimum limits for selected variables. Enclose variable names in quotation marks or apostrophes. [Example:]{.underline} list("DURATION"=0.1, "FREQMEAN"=1000, "FREQRANGE"=500)

-   **filterVars_max** \| list of maximum limits for selected variables. Enclose variable names in quotation marks or apostrophes. [Example:]{.underline} list("DURATION"=5.0, "FREQMEAN"=25000, "FREQRANGE"=15000)

**Adjust these parameters below and run the cell to read in your data.**

This cell provides output in two different panels, which you can click between. The first shows a table of all variables identified in your data along with their average values for each category of your first hierarchical variable. The second panels provides a summary of the data loaded.

```{r}
root_directory <- "UKData"
from_folders <- TRUE
levels <- c('species', 'location', 'encounter', 'recording')

vocType = 'whistle'
omitVars <- c('DCQUARTER1MEAN', 'DCQUARTER2MEAN', 'DCQUARTER3MEAN',
              'DCQUARTER4MEAN', 'DCMEAN', 'DCSTDDEV', 'RMSSIGNAL')

filterVars_min <- list()
filterVars_max <- list()

info <- loadDataFromHier(root_directory, from_folders=from_folders, 
                         levels=levels, vocType=vocType, omitVars=omitVars,
                         filterVarsMin = filterVars_min,
                         filterVarsMax = filterVars_max)

allData <- info[[1]]
variables <- info[[2]]

```

## üîé (2) Explore your data

**Customize and run the cell below to visualize your data according to different variables.**

The following parameters can be adjusted:

-   **VARIABLE1** \| name of first selected variable to include in plot.

-   **VARIABLE2** \| name of second selected variable to include in plot. If you only want to analyse one variable, leave VARIABLE2 as NULL.

-   **targetVar** \| variable to color-code plot by (grouping variable)

-   **alpha** \| transparency of scatter points (0-1)

-   **size** \| size of scatter points

-   **export** \| save plots to folder (TRUE or FALSE)

-   **resultsFolder** \| specific folder to save plots to. To create new folder, leave resultsFolder as NULL.

```{r}
VARIABLE1 <- 'FREQBEG'
VARIABLE2 <- 'FREQEND'
targetVar <- 'species'
alpha = 0.5
size = 2
export = FALSE
resultsFolder = NULL


dataPlot(d=allData, 
         variables=list('x'=VARIABLE1, 'y'=VARIABLE2, 'group'=targetVar),
         alpha=alpha, size=size, export=export, resultsFolder=resultsFolder)
```

## üìä (3) Train and test classifier models

**Train a classifier model on your data.** The next code cell is for training a classifier model using your data. Here, several parameters can be adjusted to tweak the design and training of your model:

-   **targetVar** \| target variable for classification. [Example:]{.underline} 'species'

-   **groupVar** \| variable for grouping data prior to train-test split. [Example:]{.underline} 'encounter'

-   **groupMax** \| maximum number of training examples per group. [Example:]{.underline} 50

-   **pruneTrain** \| proportion (0-1) of training data to prune out using PCA-based pruning (see supplementary information for more detail). [Example:]{.underline} 0.10

-   **minScore** \| minimum decision score (0-1) for keeping classifications (see supplementary information for more detail). [Example:]{.underline} 0.05

-   **select** \| vector of select groups to restrict training and testing to (leave as c() to not select any). [Example:]{.underline} c("Location1", "Location2")

-   **omit** \| list of categories for any hierarchical level in data to exclude from classifier training and testing. [Example:]{.underline} list('species'=c('SpeciesA"), 'location'=c('Location2", "Location3"))

```{r}
#| warning: false
targetVar <- 'species'
groupVar <- 'encounter'
nMax <- 50
pruneTrain <- 0
minScore <- 0.02
select_groups <- c()
omit <- list()



info <- classifyData(allData, vars=variables, targetVar=targetVar, 
                     groupVar=groupVar, nMax=nMax, prune=pruneTrain, 
                     minScore=minScore, select_groups=select_groups, omit=omit)

groupPreds <- info$groupPreds
allPreds <- info$allPreds
model <- info$model
```

**Run the cell below to output a summary of your classifier training and testing.**

Again, the output is shown in two panels below. The first panel gives a confusion matrix table of true labels against predicted labels, where correct classifications are shown along the diagonal. The second panel gives a written summary of overall and mean classification accuracy.

```{r}
summResults(groupPreds, targetVar=targetVar, minScore=0.0)
```

**Visualize classifier performance by running the below cell.**

The three output panels here show different aspect of the classification results. The first (left) panel shows variable importance in terms of Gini impurity decrease of the 15 most important variables used by the model. The second (middle) panel shows a scatter plot of classification accuracy against the % of predictions discarded using increasing minimum decision score thresholds. The third (right) panel shows overall accuracy, mean accuracy, and % of predictions classified at increasing minimum decision score thresholds.

```{r}
#| fig-height: 6
#| fig-width: 9
plot_info <- plotResults(groupPreds, model=model,
                         thrMax=0.10, point_size=1, targetVar=targetVar)

```

## üì• (4) Export results

Run the cell below to export your data, results, and figures. These will be saved to a **classificationResults** folder in the same directory as this notebook containing the following items:

-   **figures** \| folder containing your classification results plots

-   **classifier.rds** \| Random Forest classifier model

-   **allData.csv** \| Table of individual classifications

-   **allPredictions.csv** \| Table of individual classifications and their acoustic variables

-   **groupPredictions.csv** \| Table of group classifications

```{r}
dirName <- makeDirSysDT(create=FALSE)
if (!dirName %in% dir()) {
  dirName <- makeDirSysDT(create=TRUE)
}
if (!'figures' %in% dir(dirName)) {
  dir.create(sprintf('%s/figures', dirName))
}

if (exists('allData')) {
  write.csv(allData, sprintf('%s/allData.csv', dirName), row.names=FALSE)
}

if (exists('groupPreds')) {
  write.csv(groupPreds, sprintf('%s/groupPredictions.csv', dirName), row.names=FALSE)
}

if (exists('allPreds')) {
  write.csv(allPreds, sprintf('%s/allPredictions.csv', dirName), row.names=FALSE)
}

if (exists('plot_info')) {
  if (is.list(plot_info)) {
    ggsave(sprintf('%s/figures/performanceThresholds.png', dirName), 
       plot=plot_info$plotOverall, width=8, height=6, units="in", dpi=300)

ggsave(sprintf('%s/figures/performanceGroups.png', dirName), 
       plot=plot_info$plotGroups, width=8, height=6, units="in", dpi=300)
  }
}

if (exists('model')) {
  saveRDS(model, sprintf('%s/classifier.rds', dirName))
}

```

## (5) In dev.

```{r}
source('Classify_functions.R')

dirSelect <- c('macronesia')

targetVar <- 'species'
groupVar <- 'eventID'
nMin <- 1

dataSelect <- list()
all_groups <- c()
for (i in 1:length(dirSelect)) {
  d <- read.csv(sprintf('%s/groupPredictions1.csv', dirSelect[i]))
  #d <- subset(d, n >= nMin)
  all_groups <- append(all_groups, unique(d[[groupVar]]))
  all_groups <- unique(all_groups)[order(unique(all_groups))]
  dataSelect[[LETTERS[i]]] <- d
}

data_cmb <- data.frame()
for (group in all_groups) {
  row <- combineResults(dataSelect, group=group,
                      targetVar=targetVar, groupVar=groupVar,
                      fillValue = 'noise')
  data_cmb <- rbind(data_cmb, row)
  rownames(data_cmb) <- 1:nrow(data_cmb)
}


```

```{r}
base <- 'I:/MADEIRA_DolphinsRecs_Julie'
metadata <- data.frame()
count <- 1
for (sp in c('Dd', 'Gg', 'Gma', 'Sb', 'Sfr', 'Tt')) {
  for (enc in dir(sprintf('%s/%s', base, sp))) {
    items <- dir(sprintf('%s/%s/%s', base, sp, enc))
    if (length(items) > 0) {
      for (i in 1:length(items)) {
        row <- data.frame(id=count, species=sp, encounter=enc, filename=items[i], eventID=substr(items[i], 1, (nchar(items[i])-4)))
        metadata <- rbind(metadata, row)
      }
    }
  }
}
```

```{r}
newLabs <- c()
encID <- c()
for (i in 1:nrow(data_cmb)) {
  group <- data_cmb[[groupVar]][i]
  target <- data_cmb[[targetVar]][i]
  metasub <- subset(metadata, metadata$filename==group)
  metaSp <- metasub$species[1]
  metaEnc <- metasub$encounter[1]
  newLabs <- append(newLabs, metaSp)
  encID <- append(encID, metaEnc)
}

data_cmb$species <- newLabs
data_cmb$encID <- encID
data_cmb <- subset(data_cmb, !eventID %in% events_omit)
```

```{r}
library(ggplot2)
library(RColorBrewer)

data_paths <- list('West Africa'='I:/Nigeria_Set_Download/groupPredictions1.csv',
                   'Macronesia'='I:/testClassifiers/macronesia/groupPredictions1.csv',
                   'NE Atlantic'='C:/Users/tk81/Downloads/combined_all_edit_2805.csv',
                   'Stellwagen'='I:/Stellwagen/AllEvents2025-05-30_DD10-150_N10_edit.csv',
                   'DCASE'='I:/DCASE2025/AllEvents2025-05-30_edit1.csv')

select_data_path <- data_paths[['DCASE']]
data_cmb <- read.csv(select_data_path, fill=TRUE)

events_omit <- c('HB2102_500kHz_20210623_185156_668.wav',
                 'HB2102_500kHz_20210804_190527_714.wav',
                 'HB2102_500kHz_20210804_205527_866.wav',
                 'HB2102_500kHz_20210814_144846_028.wav')

events_omit_train <- c()

omitTarget <- c('public-square')

minEx <- 1
data_cmb <- subset(data_cmb, clicks >= minEx | whistles >= minEx)
#|

targetVar <- 'location'
groupVar <- 'eventID'
data_cmb <- subset(data_cmb, !eventID %in% events_omit)
#data_cmb <- subset(data_cmb, !data_cmb[[targetVar]] %in% omitTarget)
#data_cmb[[targetVar]] <- ifelse(data_cmb[[targetVar]] != "Tt", "Other", data_cmb[[targetVar]])

#data_cmb$species <- gsub("Sco|Sfr", "Stenella", data_cmb$species)
#data_cmb$species <- gsub("Dd|Sfr", "DdSf", data_cmb$species)

data_cmb <- subset(data_cmb, species == 'metro')
targets <- unique(data_cmb[[targetVar]])
groups <- unique(data_cmb[[groupVar]])
ind1 <- which(names(data_cmb)=='VAR6')
ind2 <- which(names(data_cmb)=='VAR12')
vars <- names(data_cmb)[ind1:ind2]

pca_result <- prcomp(data_cmb[, vars], scale. = FALSE)
pc_data <- as.data.frame(pca_result$x[, 1:2])
colnames(pc_data) <- c("PC1", "PC2")
data_pca <- cbind(data_cmb, pc_data)

ggplot(data_pca, aes(x = PC1, y = PC2, color = .data[[targetVar]])) +
  geom_point() +
  labs(x = "(PC1)",
       y = "(PC2)") +
  theme_minimal() +
  scale_color_brewer(palette = "Set2")
```

```{r}
library(randomForest)

mtry <- NULL
ns <- 10
nTrees <- 1000

groupCount <- 0
minEx <- 0
results_cmb <- data.frame()
for (k in 1:nrow(data_cmb)) {
  groupCount <- groupCount + 1
  groupTest <- data_cmb[[groupVar]][k]
  xtest <- data_cmb[k,]
  if (xtest$clicks >= minEx | xtest$whistles >= minEx) {
    xtrain <- subset(data_cmb, data_cmb[[groupVar]]!=groupTest)
    xtrain <- subset(xtrain, !xtrain$eventID %in% events_omit_train)
#    xtrain <- imputeData(xtrain, targetVar=targetVar, groupVar=groupVar, impLim=3)
#    xtrain <- pruneData(xtrain, targetVar=targetVar, vars=vars, prune=0.15)
    if (length(unique(xtrain[[targetVar]])) != length(targets)) {
      xtrain <- subset(data_cmb, data_cmb[[groupVar]]!=groupTest)
    }
    
    sampsizes <- rep(min(table(xtrain[[targetVar]])), 
                     length(unique(xtrain[[targetVar]])))
    
    formula_str <- paste(sprintf("as.factor(%s) ~", targetVar), 
                         paste(vars, collapse = " + "))
    
    if (is.null(mtry)) {
      mtry <- floor(sqrt(length(vars)))
    }
    
    m <- randomForest(as.formula(formula_str),
                    data = xtrain,
                    ntree = nTrees,
                    mtry = mtry, 
                    nodesize=ns,
                    strata = as.factor(xtrain[[targetVar]]),
                    sampsize = sampsizes,
                    na.action = na.roughfix,
                    keep.inbag = TRUE)
    
    label <- xtest[[targetVar]][1]
    predFrame <- as.numeric(predict(m, xtest, type='prob'))
    targets <- colnames(predict(m, xtest, type='prob'))
    pred <- targets[which.max(predFrame)]
    predFrame <- predFrame[rev(order(predFrame))]
    conf <- predFrame[1]
    prom <- predFrame[1] - predFrame[2]
    score <- conf*prom
    row <- list()
    row[['id']] <- groupCount
    row[[targetVar]] <- label
    row[[groupVar]] <- groupTest
    row[['eventID']] <- xtest$eventID[1]
    row[['pred']] <- pred
    row[['score']] <- score
    
    predFrame <- as.numeric(predict(m, xtest, type='prob'))
    for (i in 1:length(predFrame)) {
      row[[targets[i]]] <- predFrame[i]
    }
    
    row <- as.data.frame(row)
    
    results_cmb <- rbind(results_cmb, row)
    rownames(results_cmb) <- 1:nrow(results_cmb)
  }
}
```

```{r}
thrList <- seq(0,0.2,0.01)
thrResults <- data.frame()

count <- 1
for (thr in thrList) {
  sub <- subset(results_cmb, score >= thr)
  accs <- c()
  for (t in unique(results_cmb[[targetVar]])) {
    subsub <- subset(sub, sub[[targetVar]]==t)
    accs <- append(accs, sum(subsub$pred == subsub[[targetVar]])/nrow(subsub))
  }
  ovracc <- sum(sub$pred == sub[[targetVar]])/nrow(sub)
  disc <- 1 - nrow(sub)/nrow(results_cmb)
  row <- data.frame(id=count, minScore=thr, accOvr=ovracc, accMean=mean(accs), pDisc=disc)
  thrResults <- rbind(thrResults, row)
  rownames(thrResults) <- 1:nrow(thrResults)
  count <- count + 1
}

thrResults
```

```{r}
thr <- 0.0

for (sp in unique(results_cmb[[targetVar]])) {
  sub <- subset(results_cmb, score >= thr & results_cmb[[targetVar]]==sp)
  acc <- sum(sub$pred == sub[[targetVar]])/nrow(sub)
  cat(sprintf('%s - %s\n', sp, round(acc, 3)))
}
```

```{r}
#| fig-height: 6
#| fig-width: 9
plot_info <- plotResults(results_cmb, model=m,
                         thrMax=0.30, point_size=1, targetVar=targetVar)
```

```{r}
# 2. Extract a single tree (e.g., the first tree)
tree_num <- 15 # Let's pick the first tree
tree_info <- getTree(m, k = tree_num, labelVar = TRUE)

# --- Data Preparation for Plotting ---
# (This is the crucial part that was missing before)

# Initialize data frames for nodes and edges
nodes_df <- data.frame(
  id = 1:nrow(tree_info),
  label = character(nrow(tree_info)),
  is_terminal = tree_info$status == -1,
  prediction = tree_info$prediction
)

edges_df <- data.frame(
  from = integer(),
  to = integer(),
  label = character()
)

# Populate nodes and edges
for (i in 1:nrow(tree_info)) {
  node <- tree_info[i, ]

  # Node label based on whether it's a terminal node or a split node
  if (node$status == -1) { # Terminal node
    nodes_df$label[i] <- paste0("Pred: ", node$prediction)
  } else { # Split node
    nodes_df$label[i] <- paste0(node$`split var`, " <= ", round(node$`split point`, 2))

    # Add edges
    if (node$`left daughter` > 0) {
      edges_df <- rbind(edges_df, data.frame(
        from = i,
        to = node$`left daughter`,
        label = "True" # Or 'Yes', '<= SplitPoint'
      ))
    }
    if (node$`right daughter` > 0) {
      edges_df <- rbind(edges_df, data.frame(
        from = i,
        to = node$`right daughter`,
        label = "False" # Or 'No', '> SplitPoint'
      ))
    }
  }
}

# Remove prediction for non-terminal nodes (they don't have a final prediction)
nodes_df$prediction[!nodes_df$is_terminal] <- NA


# 3. Create an igraph object
# Ensure 'from' and 'to' in edges_df refer to valid node IDs in nodes_df
tree_graph <- graph_from_data_frame(d = edges_df, vertices = nodes_df, directed = TRUE)


plot_tree <- ggraph(tree_graph, layout = 'tree') +
  geom_edge_link(aes(label = label),
                 arrow = arrow(length = unit(3, 'mm'), type = "closed"),
                 end_cap = circle(2, 'mm'),
                 start_cap = circle(2, 'mm'),
                 color = "gray30",
                 label_colour = "darkblue",
                 label_size = 3) +
  geom_node_point(aes(color = is_terminal), size = 5) + 
  geom_node_text(aes(label = label), repel = TRUE, size = 3, bg.colour = "white", bg.r = 0.1) +
  scale_color_manual(values = c("FALSE" = "skyblue", "TRUE" = "darkgreen")) +
  theme_void() +
  labs(title = paste("Decision Tree", tree_num, "from Random Forest Model")) +
  theme(legend.position = "none") # Hide legend for simplicity

print(plot_tree)
```
